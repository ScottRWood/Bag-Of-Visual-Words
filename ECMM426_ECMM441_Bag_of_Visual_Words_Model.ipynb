{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgBiyYhhmecj"
   },
   "source": [
    "<H1 style=\"text-align: center\">ECMM426 - Computer Vision / ECMM441 - Machine Vision (Professional)</H1>\n",
    "<H2 style=\"text-align: center\">Coursework 2</H2>\n",
    "<H2 style=\"text-align: center\">Bag of Visual Words Model</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVTNQUrBmva-"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this coursework is to develop an image recognition algorithm. Specifically, we will examine the task of scene recognition starting with very simple methods -- tiny images and nearest neighbor classification -- and then move on to more advanced methods -- bags of quantized local features and linear classifiers learned by support vector machines.\n",
    "\n",
    "Bag of words models are a popular technique for image classification inspired by models used in natural language processing. The model ignores or downplays word arrangement (spatial information in the image) and classifies based on a histogram of the frequency of visual words. The visual word \"vocabulary\" is established by clustering a large corpus of local features. See Szeliski chapter 14.4.1 for more details on category recognition with quantized features. In addition, 14.3.2 discusses vocabulary creation and 14.1 covers classification techniques.\n",
    "\n",
    "For this project you will be implementing a basic bag of words model. You will classify scenes into one of 15 categories by training and testing on the 15 scene database (introduced in [Lazebnik et al. 2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf), although built on top of previously published datasets). [Lazebnik et al. 2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf) is a great paper to read, although we will be implementing the baseline method the paper discusses (equivalent to the zero level pyramid) and not the more sophisticated spatial pyramid (which is extra credit). For an excellent survey of pre-deep-learning feature encoding methods for bag of visual words models see [Chatfield et al. 2011](http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/).\n",
    "\n",
    "You are required to implement 2 different image representations -- **tiny images** and **bags of visual features** -- and 2 different classification techniques -- **nearest neighbor** and **(non-)linear SVM**. In the writeup, you are specifically asked to report performance for the following combinations, and it is also highly recommended that you implement them in this order:\n",
    "\n",
    "* Tiny images representation and nearest neighbor classifier (accuracy of about 18-25%).\n",
    "* Bag of visual words representation and nearest neighbor classifier (accuracy of about 50-60%).\n",
    "* Bag of visual words representation and linear SVM classifier (accuracy of about 60-70%).\n",
    "\n",
    "## Submission\n",
    "\n",
    "* Code (this notebook after completion) **[10 + 10 + 15 + 15 + 10 + 20 = 80 marks]**\n",
    "* Report of maximum 4 pages **[20 marks]**\n",
    "\n",
    "Note that an electronic submission is required. You have to submit the **folder** (because of some intermediate and final results) containing this notebook and the PDF report (zipped) electronically at e-BART/ELE. Please have a look on the corresponding module ELE pages for more details on the submission link/page. Any additional files needed to run this Jupyter notebook should be kept within the folder.\n",
    "\n",
    "**Note: you must also submit your cover sheet and E-Submit receipt via BART to complete the submission.**\n",
    "\n",
    "**Acknowledgement:** This coursework is adapted from [Prof James Hays](https://www.cc.gatech.edu/~hays/) of Georgia Institute of Technology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_dKhGKjogyV"
   },
   "source": [
    "Although you will undoubtedly work collaboratively in the workshops themselves, these are **individual** exercises.  The reports you write should be about the results **you** obtained, and your attention is drawn to the College and University guidelines on collaboration and plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95yiCFgZsi68"
   },
   "source": [
    "## Utils\n",
    "First, lets have some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O1k6zX0zmE09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import skimage\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from skimage import io\n",
    "from random import shuffle\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Skimage gives us some lossy conversion errors that we really don't care about\n",
    "# so we suppress them\n",
    "warnings.filterwarnings('ignore', '', UserWarning)\n",
    "\n",
    "# Double to single precision\n",
    "def im2single(im):\n",
    "    im = im.astype(np.float32) / 255\n",
    "    return im\n",
    "\n",
    "# Single to double precision\n",
    "def single2im(im):\n",
    "    im *= 255\n",
    "    im = im.astype(np.uint8)\n",
    "    return im\n",
    "\n",
    "# Load image\n",
    "def load_image(path):\n",
    "    return im2single(cv2.imread(path))[:, :, ::-1]\n",
    "\n",
    "# Load image in grayscale\n",
    "def load_image_gray(path):\n",
    "    img = load_image(path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Return image paths\n",
    "def get_image_paths(data_path, categories, num_train_per_cat=100, fmt='jpg'):\n",
    "    \"\"\"\n",
    "    This function returns lists containing the file path for each train\n",
    "    and test image, as well as listss with the label of each train and\n",
    "    test image. By default all four of these arrays will have 1500\n",
    "    elements where each element is a string.\n",
    "    :param data_path: path to the 'test' and 'train' directories\n",
    "    :param categories: list of category names\n",
    "    :param num_train_per_cat: max number of training images to use (per category)\n",
    "    :param fmt: file extension of the images\n",
    "    :return: lists: train_image_paths, test_image_paths, train_labels, test_labels\n",
    "    \"\"\"\n",
    "    train_image_paths = []\n",
    "    test_image_paths = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    for cat in categories:\n",
    "        # train\n",
    "        pth = osp.join(data_path, 'train', cat, '*.{:s}'.format(fmt))\n",
    "        pth = glob.glob(pth)\n",
    "        shuffle(pth)\n",
    "        pth = pth[:num_train_per_cat]\n",
    "        train_image_paths.extend(pth)\n",
    "        train_labels.extend([cat]*len(pth))\n",
    "\n",
    "        # test\n",
    "        pth = osp.join(data_path, 'test', cat, '*.{:s}'.format(fmt))\n",
    "        pth = glob.glob(pth)\n",
    "        shuffle(pth)\n",
    "        pth = pth[:num_train_per_cat]\n",
    "        test_image_paths.extend(pth)\n",
    "        test_labels.extend([cat]*len(pth))\n",
    "\n",
    "    return train_image_paths, test_image_paths, train_labels, test_labels\n",
    "\n",
    "# Show results\n",
    "def show_results(train_image_paths, test_image_paths, train_labels, test_labels,\n",
    "                 categories, abbr_categories, predicted_categories):\n",
    "    \"\"\"\n",
    "    shows the results\n",
    "    :param train_image_paths:\n",
    "    :param test_image_paths:\n",
    "    :param train_labels:\n",
    "    :param test_labels:\n",
    "    :param categories:\n",
    "    :param abbr_categories:\n",
    "    :param predicted_categories:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    # confusion matrix\n",
    "    y_true = [cat2idx[cat] for cat in test_labels]\n",
    "    y_pred = [cat2idx[cat] for cat in predicted_categories]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype(np.float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "    acc = np.mean(np.diag(cm))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('jet'))\n",
    "    plt.title('Confusion matrix. Mean of diagonal = {:4.2f}%'.format(acc*100))\n",
    "    tick_marks = np.arange(len(categories))\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(tick_marks, abbr_categories, rotation=45)\n",
    "    plt.yticks(tick_marks, categories)\n",
    "\n",
    "# Function creating webpage\n",
    "def create_results_webpage(train_image_paths, test_image_paths,\n",
    "\ttrain_labels, test_labels,\n",
    "\tcategories, abbr_categories, predicted_categories):\n",
    "\n",
    "\t'''\n",
    "\tStarter code prepared by James Hays for CSCI 1430 Computer Vision\n",
    "\tConverted to Python by Brendan Walsh\n",
    "\n",
    "\tThis function creates a webpage (html and images) visualizing the\n",
    "\tclassiffication results. This webpage will contain:\n",
    "\t (1) A confusion matrix plot\n",
    "\t (2) A table with one row per category, with 4 columns - training\n",
    "\t\t examples, true positives, false positives, and false negatives.\n",
    "\n",
    "\tFalse positives are instances claimed as that category but belonging to\n",
    "\tanother category, e.g. in the 'forest' row an image that was classified\n",
    "\tas 'forest' but is actually 'mountain'. This same image would be\n",
    "\tconsidered a false negative in the 'mountain' row, because it should have\n",
    "\tbeen claimed by the 'mountain' classifier but was not.\n",
    "\n",
    "\tThis webpage is similar to the one created for the SUN database in\n",
    "\t2010: http://people.csail.mit.edu/jxiao/SUN/classification397.html\n",
    "\t'''\n",
    "\n",
    "\tprint('Creating results_webpage/index.html, thumbnails, and confusion matrix.')\n",
    "\n",
    "\t# Number of examples of training examples, true positives, false positives,\n",
    "\t# and false negatives. Thus the table will be num_samples * 4 images wide\n",
    "\t# (unless there aren't enough images)\n",
    "\tnum_samples = 2\n",
    "\tthumbnail_height = 75 #pixels\n",
    "\tnum_categories = len(categories)\n",
    "\n",
    "\t# Convert everything over to numpy arrays\n",
    "\tcategories = np.array(categories)\n",
    "\tpredicted_categories = np.array(predicted_categories)\n",
    "\ttrain_labels = np.array(train_labels)\n",
    "\ttest_labels = np.array(test_labels)\n",
    "\n",
    "\t# Delete the old thumbnails, if there are any\n",
    "\tfiles = glob.glob('results_webpage/thumbnails/*.jpg')\n",
    "\tfor f in files:\n",
    "\t\tos.remove(f)\n",
    "\n",
    "\tif not os.path.isdir('results_webpage'):\n",
    "\t\tprint('Making results_webpage directory.')\n",
    "\t\tos.mkdir('results_webpage')\n",
    "\tif not os.path.isdir('results_webpage/thumbnails'):\n",
    "\t\tprint('Making thumbnails directory.')\n",
    "\t\tos.mkdir('results_webpage/thumbnails')\n",
    "\n",
    "\t### Create And Save Confusion Matrix ###\n",
    "\t# Based on the predicted category for each test case, we will now build a\n",
    "\t# confusion matrix. Entry (i,j) in this matrix well be the proportion of\n",
    "\t# times a test image of ground truth category i was predicted to be\n",
    "\t# category j. An identity matrix is the ideal case. You should expect\n",
    "\t# roughly 50-95% along the diagonal depending on your features,\n",
    "\t# classifiers, and particular categories. For example, suburb is very easy\n",
    "\t# to recognize.\n",
    "\twith open('results_webpage/index.html', 'w+') as f:\n",
    "\n",
    "\t\t# Initialize the matrix\n",
    "\t\tconfusion_matrix = np.zeros((num_categories, num_categories))\n",
    "\n",
    "\t\t# Iterate over predicted results (this is like, several hundred items long)\n",
    "\t\tfor i,cat in enumerate(predicted_categories):\n",
    "\t\t\t# Find the row and column corresponding to the label of this entry\n",
    "\t\t\t# The row is the ground truth label and the column is the found label\n",
    "\t\t\trow = np.argwhere(categories == test_labels[i])[0][0]\n",
    "\t\t\tcolumn = np.argwhere(categories == predicted_categories[i])[0][0]\n",
    "\n",
    "\t\t\t# Add 1 to the matrix for that row/col\n",
    "\t\t\t# This way we build up a histogram from our labeled data\n",
    "\t\t\tconfusion_matrix[row][column] += 1;\n",
    "\n",
    "\t\t# If the number of training examples and test cases are not equal, this\n",
    "\t\t# statement will be invalid!\n",
    "\t\t# TODO: That's an old comment left over from the matlab code that I don't\n",
    "\t\t# think still applies\n",
    "\t\tnum_test_per_cat = len(test_labels) / num_categories\n",
    "\t\tconfusion_matrix = confusion_matrix / float(num_test_per_cat)\n",
    "\t\taccuracy = np.mean(np.diag(confusion_matrix))\n",
    "\n",
    "\t\tprint('Accuracy (mean of diagonal of confusion matrix) is {:2.3%}'.format(accuracy))\n",
    "\n",
    "\t\t# plasma is the most easily-interpreted color map I've found so far\n",
    "\t\tplt.imshow(confusion_matrix, cmap='plasma', interpolation='nearest')\n",
    "\n",
    "\t\t# We put the shortened labels (e.g. \"sub\" for \"suburb\") on the x axis\n",
    "\t\tlocs, labels = plt.xticks()\n",
    "\t\tplt.xticks(np.arange(num_categories), abbr_categories)\n",
    "\n",
    "\t\t# Full labels go on y\n",
    "\t\tlocs, labels = plt.yticks()\n",
    "\t\tplt.yticks(np.arange(num_categories), categories)\n",
    "\n",
    "\t\t# Save the result\n",
    "\t\tplt.savefig('results_webpage/confusion_matrix.png', bbox_inches='tight')\n",
    "\n",
    "\t\t## Create webpage header\n",
    "\t\tf.write('<!DOCTYPE html>\\n');\n",
    "\t\tf.write('<html>\\n');\n",
    "\t\tf.write('<head>\\n');\n",
    "\t\tf.write('<link href=''http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono'' rel=''stylesheet'' type=''text/css''>\\n');\n",
    "\t\tf.write('<style type=\"text/css\">\\n');\n",
    "\n",
    "\t\tf.write('body {\\n');\n",
    "\t\tf.write('  margin: 0px;\\n');\n",
    "\t\tf.write('  width: 100%;\\n');\n",
    "\t\tf.write('  font-family: ''Crimson Text'', serif;\\n');\n",
    "\t\tf.write('  background: #fcfcfc;\\n');\n",
    "\t\tf.write('}\\n');\n",
    "\t\tf.write('table td {\\n');\n",
    "\t\tf.write('  text-align: center;\\n');\n",
    "\t\tf.write('  vertical-align: middle;\\n');\n",
    "\t\tf.write('}\\n');\n",
    "\t\tf.write('h1 {\\n');\n",
    "\t\tf.write('  font-family: ''Nunito'', sans-serif;\\n');\n",
    "\t\tf.write('  font-weight: normal;\\n');\n",
    "\t\tf.write('  font-size: 28px;\\n');\n",
    "\t\tf.write('  margin: 25px 0px 0px 0px;\\n');\n",
    "\t\tf.write('  text-transform: lowercase;\\n');\n",
    "\t\tf.write('}\\n');\n",
    "\t\tf.write('.container {\\n');\n",
    "\t\tf.write('  margin: 0px auto 0px auto;\\n');\n",
    "\t\tf.write('  width: 1160px;\\n');\n",
    "\t\tf.write('}\\n');\n",
    "\n",
    "\t\tf.write('</style>\\n');\n",
    "\t\tf.write('</head>\\n');\n",
    "\t\tf.write('<body>\\n\\n');\n",
    "\n",
    "\t\tf.write('<div class=\"container\">\\n\\n\\n');\n",
    "\t\tf.write('<center>\\n');\n",
    "\t\tf.write('<h1>Scene classification results visualization</h1>\\n');\n",
    "\t\tf.write('<img src=\"confusion_matrix.png\">\\n\\n');\n",
    "\t\tf.write('<br>\\n');\n",
    "\t\tf.write('Accuracy (mean of diagonal of confusion matrix) is %2.3f\\n' % (accuracy));\n",
    "\t\tf.write('<p>\\n\\n');\n",
    "\n",
    "\t\t## Create results table\n",
    "\t\tf.write('<table border=0 cellpadding=4 cellspacing=1>\\n');\n",
    "\t\tf.write('<tr>\\n');\n",
    "\t\tf.write('<th>Category name</th>\\n');\n",
    "\t\tf.write('<th>Accuracy</th>\\n');\n",
    "\t\tf.write('<th colspan=%d>Sample training images</th>\\n' % num_samples);\n",
    "\t\tf.write('<th colspan=%d>Sample true positives</th>\\n' % num_samples);\n",
    "\t\tf.write('<th colspan=%d>False positives with true label</th>\\n' % num_samples);\n",
    "\t\tf.write('<th colspan=%d>False negatives with wrong predicted label</th>\\n' % num_samples);\n",
    "\t\tf.write('</tr>\\n');\n",
    "\n",
    "\t\tfor i,cat in enumerate(categories):\n",
    "\t\t\tf.write('<tr>\\n');\n",
    "\n",
    "\t\t\tf.write('<td>'); #category name\n",
    "\t\t\tf.write('%s' % cat);\n",
    "\t\t\tf.write('</td>\\n');\n",
    "\n",
    "\t\t\tf.write('<td>'); # category accuracy\n",
    "\t\t\tf.write('%.3f' % confusion_matrix[i][i]);\n",
    "\t\t\tf.write('</td>\\n');\n",
    "\n",
    "\t\t\t# Collect num_samples random paths to images of each type.\n",
    "\t\t\t# Training examples.\n",
    "\t\t\ttrain_examples = np.take(train_image_paths, np.argwhere(train_labels == cat))\n",
    "\n",
    "\t\t\t# True positives. There might not be enough of these if the classifier\n",
    "\t\t\t# is bad\n",
    "\t\t\ttrue_positives = np.take(test_image_paths, np.argwhere(np.logical_and(test_labels == cat, predicted_categories == cat)))\n",
    "\n",
    "\t\t\t# False positives. There might not be enough of them if the classifier\n",
    "\t\t\t# is good\n",
    "\t\t\tfalse_positive_inds = np.argwhere(np.logical_and(np.invert(cat == test_labels), cat == predicted_categories))\n",
    "\t\t\tfalse_positives = np.take(test_image_paths, false_positive_inds)\n",
    "\t\t\tfalse_positive_labels = np.take(test_labels, false_positive_inds)\n",
    "\n",
    "\t\t\t# False negatives. There might not be enough of them if the classifier\n",
    "\t\t\t# is good\n",
    "\t\t\tfalse_negative_inds = np.argwhere(np.logical_and(cat == test_labels, np.invert(cat == predicted_categories)))\n",
    "\t\t\tfalse_negatives = np.take(test_image_paths, false_negative_inds)\n",
    "\t\t\tfalse_negative_labels = np.take(predicted_categories, false_negative_inds)\n",
    "\n",
    "\t\t\t# Randomize each list of files\n",
    "\t\t\tnp.random.shuffle(train_examples)\n",
    "\t\t\tnp.random.shuffle(true_positives)\n",
    "\n",
    "\t\t\t# HACK: Well, sort of a hack. We need to shuffle the false_positives\n",
    "\t\t\t# and their labels in the same exact order, so we get the RNG state,\n",
    "\t\t\t# save it, shuffle, restore, then shuffle the other list so that they\n",
    "\t\t\t# shuffle in tandem.\n",
    "\t\t\trng_state = np.random.get_state()\n",
    "\t\t\tnp.random.shuffle(false_positives)\n",
    "\t\t\tnp.random.set_state(rng_state)\n",
    "\t\t\tnp.random.shuffle(false_positive_labels)\n",
    "\n",
    "\t\t\trng_state = np.random.get_state()\n",
    "\t\t\tnp.random.shuffle(false_negatives)\n",
    "\t\t\tnp.random.set_state(rng_state)\n",
    "\t\t\tnp.random.shuffle(false_negative_labels)\n",
    "\n",
    "\t\t\t# Truncate each list to be at most num_samples long\n",
    "\t\t\ttrain_examples  = train_examples[0:min(len(train_examples), num_samples)]\n",
    "\t\t\ttrue_positives  = true_positives[0:min(len(true_positives), num_samples)]\n",
    "\t\t\tfalse_positives = false_positives[0:min(len(false_positives), num_samples)]\n",
    "\t\t\tfalse_positive_labels = false_positive_labels[0:min(len(false_positive_labels),num_samples)]\n",
    "\t\t\tfalse_negatives = false_negatives[0:min(len(false_negatives),num_samples)]\n",
    "\t\t\tfalse_negative_labels = false_negative_labels[0:min(len(false_negative_labels),num_samples)]\n",
    "\n",
    "\t\t\t# Sample training images\n",
    "\t\t\t# Create and save all of the thumbnails\n",
    "\t\t\tfor j in range(num_samples):\n",
    "\t\t\t\tif j + 1 <= len(train_examples):\n",
    "\t\t\t\t\tthisExample = train_examples[j][0]\n",
    "\t\t\t\t\ttmp = skimage.io.imread(thisExample)\n",
    "\t\t\t\t\theight, width = rescale(tmp.shape, thumbnail_height)\n",
    "\t\t\t\t\ttmp = transform.resize(tmp, (height, width),\n",
    "\t\t\t\t\t\tanti_aliasing=True, mode='wrap')\n",
    "\n",
    "\t\t\t\t\tname = os.path.basename(thisExample)\n",
    "\t\t\t\t\ttmp_uint8 = (tmp * 255).astype(np.uint8)\n",
    "\t\t\t\t\tskimage.io.imsave('results_webpage/thumbnails/' + cat + '_' + name, tmp_uint8, quality=100)\n",
    "\t\t\t\t\tf.write('<td bgcolor=LightBlue>')\n",
    "\t\t\t\t\tf.write('<img src=\"%s\" width=%d height=%d>' % ('thumbnails/' + cat + '_' + name, width, height))\n",
    "\t\t\t\t\tf.write('</td>\\n')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf.write('<td bgcolor=LightBlue>')\n",
    "\t\t\t\t\tf.write('</td>\\n')\n",
    "\n",
    "\t\t\tfor j in range(num_samples):\n",
    "\t\t\t\tif j + 1 <= len(true_positives):\n",
    "\t\t\t\t\tthisExample = true_positives[j][0]\n",
    "\t\t\t\t\ttmp = skimage.io.imread(thisExample)\n",
    "\t\t\t\t\theight, width = rescale(tmp.shape, thumbnail_height)\n",
    "\t\t\t\t\ttmp = transform.resize(tmp, (height, width),\n",
    "\t\t\t\t\t\tanti_aliasing=True, mode='wrap')\n",
    "\n",
    "\t\t\t\t\tname = os.path.basename(thisExample)\n",
    "\t\t\t\t\ttmp_uint8 = (tmp * 255).astype(np.uint8)\n",
    "\t\t\t\t\tskimage.io.imsave('results_webpage/thumbnails/' + cat + '_' + name, tmp_uint8, quality=100)\n",
    "\t\t\t\t\tf.write('<td bgcolor=LightGreen>');\n",
    "\t\t\t\t\tf.write('<img src=\"%s\" width=%d height=%d>' % ('thumbnails/' + cat + '_' + name, width, height))\n",
    "\t\t\t\t\tf.write('</td>\\n');\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf.write('<td bgcolor=LightGreen>');\n",
    "\t\t\t\t\tf.write('</td>\\n');\n",
    "\n",
    "\t\t\tfor j in range(num_samples):\n",
    "\t\t\t\tif j + 1 <= len(false_positives):\n",
    "\t\t\t\t\tthisExample = false_positives[j][0]\n",
    "\t\t\t\t\ttmp = skimage.io.imread(thisExample)\n",
    "\t\t\t\t\theight, width = rescale(tmp.shape, thumbnail_height)\n",
    "\t\t\t\t\ttmp = transform.resize(tmp, (height, width),\n",
    "\t\t\t\t\t\tanti_aliasing=True, mode='wrap')\n",
    "\n",
    "\t\t\t\t\tname = os.path.basename(thisExample)\n",
    "\t\t\t\t\ttmp_uint8 = (tmp * 255).astype(np.uint8)\n",
    "\t\t\t\t\tskimage.io.imsave('results_webpage/thumbnails/' + cat + '_' + name, tmp_uint8, quality=100)\n",
    "\t\t\t\t\tf.write('<td bgcolor=LightCoral>');\n",
    "\t\t\t\t\tf.write('<img src=\"%s\" width=%d height=%d>' % ('thumbnails/' + cat + '_' + name, width, height))\n",
    "\t\t\t\t\tf.write('<br><small>%s</small>' % false_positive_labels[j][0]);\n",
    "\t\t\t\t\tf.write('</td>\\n');\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf.write('<td bgcolor=LightCoral>');\n",
    "\t\t\t\t\tf.write('</td>\\n');\n",
    "\n",
    "\t\t\tfor j in range(num_samples):\n",
    "\t\t\t\tif j + 1 <= len(false_negatives):\n",
    "\t\t\t\t\tthisExample = false_negatives[j][0]\n",
    "\t\t\t\t\ttmp = skimage.io.imread(thisExample)\n",
    "\t\t\t\t\theight, width = rescale(tmp.shape, thumbnail_height)\n",
    "\t\t\t\t\ttmp = transform.resize(tmp, (height, width),\n",
    "\t\t\t\t\t\tanti_aliasing=True, mode='wrap')\n",
    "\n",
    "\t\t\t\t\tname = os.path.basename(thisExample)\n",
    "\t\t\t\t\ttmp_uint8 = (tmp * 255).astype(np.uint8)\n",
    "\t\t\t\t\tskimage.io.imsave('results_webpage/thumbnails/' + cat + '_' + name, tmp_uint8, quality=100)\n",
    "\t\t\t\t\tf.write('<td bgcolor=#FFBB55>');\n",
    "\t\t\t\t\tf.write('<img src=\"%s\" width=%d height=%d>' % ('thumbnails/' + cat + '_' + name, width, height));\n",
    "\t\t\t\t\tf.write('<br><small>%s</small>' % false_negative_labels[j][0]);\n",
    "\t\t\t\t\tf.write('</td>\\n');\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf.write('<td bgcolor=#FFBB55>');\n",
    "\t\t\t\t\tf.write('</td>\\n');\n",
    "\n",
    "\t\t\tf.write('</tr>\\n');\n",
    "\n",
    "\t\tf.write('<tr>\\n');\n",
    "\t\tf.write('<th>Category name</th>\\n');\n",
    "\t\tf.write('<th>Accuracy</th>\\n');\n",
    "\t\tf.write('<th colspan=%d>Sample training images</th>\\n' % num_samples);\n",
    "\t\tf.write('<th colspan=%d>Sample true positives</th>\\n' % num_samples);\n",
    "\t\tf.write('<th colspan=%d>False positives with true label</th>\\n' % num_samples);\n",
    "\t\tf.write('<th colspan=%d>False negatives with wrong predicted label</th>\\n' % num_samples);\n",
    "\t\tf.write('</tr>\\n');\n",
    "\n",
    "\t\tf.write('</table>\\n');\n",
    "\t\tf.write('</center>\\n\\n\\n');\n",
    "\t\tf.write('</div>\\n');\n",
    "\n",
    "\t\t## Create end of web page\n",
    "\t\tf.write('</body>\\n');\n",
    "\t\tf.write('</html>\\n');\n",
    "\n",
    "\tprint('Wrote results page to results_webpage/index.html.')\n",
    "\n",
    "# Function to rescale\n",
    "def rescale(dims, thumbnail_height):\n",
    "\theight = dims[1]\n",
    "\tfactor = thumbnail_height / height\n",
    "\tleft = int(round(dims[0] * factor))\n",
    "\tright = int(round(dims[1] * factor))\n",
    "\treturn (left, right)\n",
    " \n",
    "# Wrapping function\n",
    "def projSceneRecBoW(feature='placeholder', classifier='placeholder', \n",
    "                    load_vocab='True', data_path='bovw15'):\n",
    "    '''\n",
    "    For this project, you will need to report performance for three\n",
    "    combinations of features / classifiers. We recommend that you code them in\n",
    "    this order:\n",
    "        1) Tiny image features and nearest neighbor classifier\n",
    "        2) Bag of word features and nearest neighbor classifier\n",
    "        3) Bag of word features and linear SVM classifier\n",
    "    The starter code is initialized to 'placeholder' just so that the starter\n",
    "    code does not crash when run unmodified and you can get a preview of how\n",
    "    results are presented.\n",
    "    '''\n",
    "\n",
    "    # Step 0: Set up parameters, category list, and image paths.\n",
    "    FEATURE = feature\n",
    "    CLASSIFIER = classifier\n",
    "\n",
    "    # This is the list of categories / directories to use. The categories are\n",
    "    # somewhat sorted by similarity so that the confusion matrix looks more\n",
    "    # structured (indoor and then urban and then rural).\n",
    "    categories = ['Kitchen', 'Store', 'Bedroom', 'LivingRoom', 'Office',\n",
    "           'Industrial', 'Suburb', 'InsideCity', 'TallBuilding', 'Street',\n",
    "           'Highway', 'OpenCountry', 'Coast', 'Mountain', 'Forest']\n",
    "\n",
    "    # This list of shortened category names is used later for visualization.\n",
    "    abbr_categories = ['Kit', 'Sto', 'Bed', 'Liv', 'Off', 'Ind', 'Sub',\n",
    "        'Cty', 'Bld', 'St', 'HW', 'OC', 'Cst', 'Mnt', 'For']\n",
    "\n",
    "    # Number of training examples per category to use. Max is 100. For\n",
    "    # simplicity, we assume this is the number of test cases per category as\n",
    "    # well.\n",
    "    num_train_per_cat = 100\n",
    "\n",
    "    # This function returns string arrays containing the file path for each train\n",
    "    # and test image, as well as string arrays with the label of each train and\n",
    "    # test image. By default all four of these arrays will be 1500x1 where each\n",
    "    # entry is a string.\n",
    "    print('Getting paths and labels for all train and test data.')\n",
    "    train_image_paths, test_image_paths, train_labels, test_labels = \\\n",
    "        get_image_paths(data_path, categories, num_train_per_cat)\n",
    "    #   train_image_paths  1500x1   list\n",
    "    #   test_image_paths   1500x1   list\n",
    "    #   train_labels       1500x1   list\n",
    "    #   test_labels        1500x1   list\n",
    "\n",
    "    ############################################################################\n",
    "    ## Step 1: Represent each image with the appropriate feature\n",
    "    # Each function to construct features should return an N x d matrix, where\n",
    "    # N is the number of paths passed to the function and d is the\n",
    "    # dimensionality of each image representation. See the starter code for\n",
    "    # each function for more details.\n",
    "    ############################################################################\n",
    "\n",
    "    print('Using %s representation for images.' % FEATURE)\n",
    "\n",
    "    if FEATURE.lower() == 'tiny_image':\n",
    "        print('Loading tiny images...')\n",
    "        # YOU CODE get_tiny_images (see section 1 above)\n",
    "        train_image_feats = get_tiny_images(train_image_paths)\n",
    "        test_image_feats  = get_tiny_images(test_image_paths)\n",
    "        print('Tiny images loaded.')\n",
    "\n",
    "    elif FEATURE.lower() == 'bag_of_words':\n",
    "        # Because building the vocabulary takes a long time, we save the generated\n",
    "        # vocab to a file and re-load it each time to make testing faster. If\n",
    "        # you need to re-generate the vocab (for example if you change its size\n",
    "        # or the length of your feature vectors), set --load_vocab to False.\n",
    "        # This will re-compute the vocabulary.\n",
    "        if load_vocab == 'True':\n",
    "            # check if vocab exists\n",
    "            if not os.path.isfile('vocab.npy'):\n",
    "                print('IOError: No existing visual word vocabulary found. Please set --load_vocab to False.')\n",
    "                exit()\n",
    "\n",
    "        elif load_vocab == 'False':\n",
    "            print('Computing vocab from training images.')\n",
    "\n",
    "            #Larger values will work better (to a point), but are slower to compute\n",
    "            vocab_size = 200\n",
    "\n",
    "            # YOU CODE build_vocabulary (see section 3 above)\n",
    "            vocab = build_vocabulary(train_image_paths, vocab_size)\n",
    "            np.save('vocab.npy', vocab)\n",
    "        else:\n",
    "            raise ValueError('Unknown load flag! Should be boolean.')\n",
    "\n",
    "        # YOU CODE get_bags_of_words.m (see section 4 above)\n",
    "        train_image_feats = get_bags_of_words(train_image_paths)\n",
    "        # You may want to write out train_image_features here as a *.npy and\n",
    "        # load it up later if you want to just test your classifiers without\n",
    "        # re-computing features\n",
    "\n",
    "        test_image_feats  = get_bags_of_words(test_image_paths)\n",
    "        # Same goes here for test image features.\n",
    "\n",
    "    elif FEATURE.lower() == 'placeholder':\n",
    "        train_image_feats = []\n",
    "        test_image_feats = []\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unknown feature type!')\n",
    "\n",
    "    ############################################################################\n",
    "    ## Step 2: Classify each test image by training and using the appropriate classifier\n",
    "    # Each function to classify test features will return an N x 1 string array,\n",
    "    # where N is the number of test cases and each entry is a string indicating\n",
    "    # the predicted category for each test image. Each entry in\n",
    "    # 'predicted_categories' must be one of the 15 strings in 'categories',\n",
    "    # 'train_labels', and 'test_labels'. See the starter code for each function\n",
    "    # for more details.\n",
    "    ############################################################################\n",
    "\n",
    "    print('Using %s classifier to predict test set categories.' % CLASSIFIER)\n",
    "\n",
    "    if CLASSIFIER.lower() == 'nearest_neighbor':\n",
    "        # YOU CODE nearest_neighbor_classify (see section 2 above)\n",
    "        predicted_categories = nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats)\n",
    "\n",
    "    elif CLASSIFIER.lower() == 'support_vector_machine':\n",
    "        # YOU CODE svm_classify (see section 6 above)\n",
    "        predicted_categories = svm_classify(train_image_feats, train_labels, test_image_feats)\n",
    "\n",
    "    elif CLASSIFIER.lower() == 'placeholder':\n",
    "        # The placeholder classifier simply predicts a random category for every test case\n",
    "        random_permutation = np.random.permutation(len(test_labels))\n",
    "        predicted_categories = [test_labels[i] for i in random_permutation]\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unknown classifier type')\n",
    "\n",
    "    ############################################################################\n",
    "    ## Step 3: Build a confusion matrix and score the recognition system\n",
    "    # You do not need to code anything in this section.\n",
    "\n",
    "    # If we wanted to evaluate our recognition method properly we would train\n",
    "    # and test on many random splits of the data. You are not required to do so\n",
    "    # for this project.\n",
    "\n",
    "    # This function will recreate results_webpage/index.html and various image\n",
    "    # thumbnails each time it is called. View the webpage to help interpret\n",
    "    # your classifier performance. Where is it making mistakes? Are the\n",
    "    # confusions reasonable?\n",
    "    ############################################################################\n",
    "\n",
    "    create_results_webpage( train_image_paths, \\\n",
    "                            test_image_paths, \\\n",
    "                            train_labels, \\\n",
    "                            test_labels, \\\n",
    "                            categories, \\\n",
    "                            abbr_categories, \\\n",
    "                            predicted_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFLhNoC8EjgN"
   },
   "source": [
    "## Data\n",
    "Download the data and get the train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Njm9dvxZtbVu",
    "outputId": "2adc9499-8d72-4e11-aa09-327c44c5acac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-13 12:03:48--  https://empslocal.ex.ac.uk/people/staff/ad735/ECMM426/bovw15.zip\n",
      "Resolving empslocal.ex.ac.uk (empslocal.ex.ac.uk)... 144.173.36.85\n",
      "Connecting to empslocal.ex.ac.uk (empslocal.ex.ac.uk)|144.173.36.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 88338399 (84M) [application/zip]\n",
      "Saving to: ‘bovw15.zip’\n",
      "\n",
      "bovw15.zip          100%[===================>]  84.25M  20.4MB/s    in 7.6s    \n",
      "\n",
      "2021-03-13 12:03:57 (11.1 MB/s) - ‘bovw15.zip’ saved [88338399/88338399]\n",
      "\n",
      "# Training images: 1500\n",
      "# Test images: 1500\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('bovw15.zip'):\n",
    "    !wget --no-check-certificate https://empslocal.ex.ac.uk/people/staff/ad735/ECMM426/bovw15.zip\n",
    "    !unzip -q bovw15.zip\n",
    "    !rm bovw15.zip\n",
    "# This is the list of categories / directories to use. The categories are\n",
    "# somewhat sorted by similarity so that the confusion matrix looks more\n",
    "# structured (indoor and then urban and then rural).\n",
    "categories = ['Kitchen', 'Store', 'Bedroom', 'LivingRoom', 'Office', 'Industrial', 'Suburb',\n",
    "              'InsideCity', 'TallBuilding', 'Street', 'Highway', 'OpenCountry', 'Coast',\n",
    "              'Mountain', 'Forest'];\n",
    "# This list of shortened category names is used later for visualization\n",
    "abbr_categories = ['Kit', 'Sto', 'Bed', 'Liv', 'Off', 'Ind', 'Sub',\n",
    "                   'Cty', 'Bld', 'St', 'HW', 'OC', 'Cst',\n",
    "                   'Mnt', 'For'];\n",
    "\n",
    "# Number of training examples per category to use. Max is 100. For\n",
    "# simplicity, we assume this is the number of test cases per category, as\n",
    "# well.\n",
    "num_train_per_cat = 100\n",
    "\n",
    "# This function returns lists containing the file path for each train\n",
    "# and test image, as well as lists with the label of each train and\n",
    "# test image. By default all four of these lists will have 1500 elements\n",
    "# where each element is a string.\n",
    "data_path = 'bovw15'\n",
    "train_image_paths, test_image_paths, train_labels, test_labels = get_image_paths(data_path,\n",
    "                                                                                 categories,\n",
    "                                                                                 num_train_per_cat)\n",
    "print('# Training images: {0}'.format(len(train_image_paths)))\n",
    "print('# Test images: {0}'.format(len(test_image_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOlCqhzdph_F"
   },
   "source": [
    "## 1. Tiny Image Features\n",
    "\n",
    "The \"tiny image\" feature, inspired by the work of the same name by [Torralba, Fergus, and Freeman](http://groups.csail.mit.edu/vision/TinyImages/), is one of the simplest possible image representations. One simply resizes each image to a small, fixed resolution (we recommend 16x16). It works slightly better if the tiny image is made to have zero mean and unit length. This is not a particularly good representation, because it discards all of the high frequency image content and is not especially invariant to spatial or brightness shifts. [Torralba, Fergus, and Freeman](http://groups.csail.mit.edu/vision/TinyImages/) propose several alignment methods to alleviate the latter drawback, but we will not worry about alignment for this project. We are using tiny images simply as a baseline. See `get_tiny_images()` code below for more details. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h0miRRbEp0vR"
   },
   "outputs": [],
   "source": [
    "def get_tiny_images(image_paths):\n",
    "    '''\n",
    "    This feature is inspired by the simple tiny images used as features in\n",
    "    80 million tiny images: a large dataset for non-parametric object and\n",
    "    scene recognition. A. Torralba, R. Fergus, W. T. Freeman. IEEE\n",
    "    Transactions on Pattern Analysis and Machine Intelligence, vol.30(11),\n",
    "    pp. 1958-1970, 2008. http://groups.csail.mit.edu/vision/TinyImages/\n",
    "\n",
    "    Inputs:\n",
    "        image_paths: a 1-D Python list of strings. Each string is a complete\n",
    "                     path to an image on the filesystem.\n",
    "        these are provisional arguments, please feel free to change if you need.\n",
    "    Outputs:\n",
    "        An n x d numpy array where n is the number of images and d is the\n",
    "        length of the tiny image representation vector. e.g. if the images\n",
    "        are resized to 16x16, then d is 16 * 16 = 256.\n",
    "\n",
    "    To build a tiny image feature, resize the original image to a very small\n",
    "    square resolution (e.g. 16x16). You can either resize the images to square\n",
    "    while ignoring their aspect ratio, or you can crop the images into squares\n",
    "    first and then resize evenly. Normalizing these tiny images will increase\n",
    "    performance modestly.\n",
    "\n",
    "    As you may recall from class, naively downsizing an image can cause\n",
    "    aliasing artifacts that may throw off your comparisons. See the docs for\n",
    "    skimage.transform.resize for details:\n",
    "    http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize\n",
    "\n",
    "    Suggested functions: skimage.transform.resize, skimage.color.rgb2grey,\n",
    "                         skimage.io.imread, np.reshape\n",
    "    '''\n",
    "    ############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                     #\n",
    "    ############################################################################\n",
    "    \n",
    "    image = skimage.transform.resize(skimage.color.rgb2grey(skimage.io.imread(image_paths[0])), (16,16))\n",
    "    image = np.reshape(image, (1,256))\n",
    "    feats = np.array(image)\n",
    "    \n",
    "    for i in image_paths[1:]:\n",
    "        image = skimage.transform.resize(skimage.color.rgb2grey(skimage.io.imread(i)), (16,16))\n",
    "        image = np.reshape(image, (1,256))\n",
    "        feats = np.concatenate([feats, image])\n",
    "        \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UIhfrKvp1bc"
   },
   "source": [
    "## 2. Nearest Neighbour Classifier\n",
    "\n",
    "The nearest neighbor classifier is equally simple to understand. When tasked with classifying a test feature into a particular category, one simply finds the \"nearest\" training example (L2 distance is a sufficient metric) and assigns the test case the label of that nearest training example. The nearest neighbor classifier has many desirable features -- it requires no training, it can learn arbitrarily complex decision boundaries, and it trivially supports multiclass problems. It is quite vulnerable to training noise (outliers), though, which can be alleviated by voting based on the K nearest neighbors (but you are not required to do so). Nearest neighbor classifiers also suffer as the feature dimensionality increases, because the classifier has no mechanism to learn which dimensions are irrelevant for the decision. The nearest neighbor computation also becomes slow for high dimensional data and many training examples. See the `nearest_neighbor_classify()` description below for more details.\n",
    "\n",
    "Together, the tiny image representation and nearest neighbor classifier will get about 15% to 25% accuracy on the 15 scene database. For comparison, chance performance is ~7%. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CszIr_K7qDrw"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mode\n",
    "\n",
    "def nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats,\n",
    "                              metric='euclidean'):\n",
    "    '''\n",
    "    This function will predict the category for every test image by finding\n",
    "    the training image with most similar features. You will complete the given\n",
    "    partial implementation of k-nearest-neighbors such that for any arbitrary\n",
    "    k, your algorithm finds the closest k neighbors and then votes among them\n",
    "    to find the most common category and returns that as its prediction.\n",
    "\n",
    "    Inputs:\n",
    "        train_image_feats: An nxd numpy array, where n is the number of training\n",
    "                           examples, and d is the image descriptor vector size.\n",
    "        train_labels: An nx1 Python list containing the corresponding ground\n",
    "                      truth labels for the training data.\n",
    "        test_image_feats: An mxd numpy array, where m is the number of test\n",
    "                          images and d is the image descriptor vector size.\n",
    "        these are provisional arguments, please feel free to change if you need.\n",
    "\n",
    "    Outputs:\n",
    "        An m x 1 numpy list of strings, where each string is the predicted label\n",
    "        for the corresponding image in test_image_feats\n",
    "\n",
    "    The simplest implementation of k-nearest-neighbors gives an even vote to\n",
    "    all k neighbors found - that is, each neighbor in category A counts as one\n",
    "    vote for category A, and the result returned is equivalent to finding the\n",
    "    mode of the categories of the k nearest neighbors. A more advanced version\n",
    "    uses weighted votes where closer matches matter more strongly than far ones.\n",
    "    This is not required, but may increase performance.\n",
    "\n",
    "    Be aware that increasing k does not always improve performance - even\n",
    "    values of k may require tie-breaking which could cause the classifier to\n",
    "    arbitrarily pick the wrong class in the case of an even split in votes.\n",
    "    Additionally, past a certain threshold the classifier is considering so\n",
    "    many neighbors that it may expand beyond the local area of logical matches\n",
    "    and get so many garbage votes from a different category that it mislabels\n",
    "    the data. Play around with a few values and see what changes.\n",
    "\n",
    "    Useful functions:\n",
    "        scipy.spatial.distance.cdist, np.argsort, scipy.stats.mode, although, \n",
    "        there is no restriction on the usability of functions here.\n",
    "    '''\n",
    "\n",
    "    k = 1\n",
    "\n",
    "    # Gets the distance between each test image feature and each train image feature\n",
    "    # e.g., cdist\n",
    "                      # ROWS            COLS\n",
    "    distances = cdist(test_image_feats, train_image_feats, metric)\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                     #\n",
    "    ############################################################################\n",
    "\n",
    "    #TODO:\n",
    "    # 1) Find the k closest features to each test image feature in euclidean space\n",
    "    # 2) Determine the labels of those k features\n",
    "    # 3) Pick the most common label from the k\n",
    "    # 4) Store that label in a list\n",
    "\n",
    "    test_labels = np.array([])\n",
    "    \n",
    "    for i in range(len(test_image_feats)):\n",
    "        dists = np.argsort(distances[i])[:k]\n",
    "        labels = [train_labels[j] for j in dists]\n",
    "        \n",
    "        label = mode(labels)[0][0]\n",
    "        test_labels = np.append(test_labels, label)\n",
    "       \n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX4Z1eJjqExu"
   },
   "source": [
    "## 3. Build Vocabulary\n",
    "\n",
    "After you have implemented a baseline scene recognition pipeline it is time to move on to a more sophisticated image representation -- bags of visual features. Before we can represent our training and testing images as bag of feature histograms, we first need to establish a vocabulary of visual words. We will form this vocabulary by sampling many local features from our training set (10's or 100's of thousands) and then clustering them with kmeans. For computing local features, you can use your favourite local feature descriptors, such as SIFT, HOG, SURF etc listed [here](http://scikit-image.org/docs/dev/api/skimage.feature.html) or the implementation from OpenCV. The number of kmeans clusters is the size of our vocabulary and the size of our features. For example, you might start by clustering many visual descriptors into k=50 clusters. This partitions the continuous, $d$ dimensional visual feature space into 50 regions. For any new visual feature we observe, we can figure out which region it belongs to as long as we save the centroids of our original clusters. Those centroids are our visual word vocabulary. Because it can be slow to sample and cluster many local features, the starter code saves the cluster centroids and avoids recomputing them during the future runs. See the `build_vocabulary()` code below for more details. **[15 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPHWCdv0XNlZ",
    "outputId": "e25a0e38-af50-4ffe-adb1-1dc7aa47600a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python==4.4.0.44 in /usr/local/lib/python3.7/dist-packages (4.4.0.44)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.4.0.44) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python==4.4.0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1oIY4zJiqSPl"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    '''\n",
    "    This function should sample descriptors from the training images,\n",
    "    cluster them with kmeans, and then return the cluster centers.\n",
    "\n",
    "    Inputs:\n",
    "        image_paths: a Python list of image path strings\n",
    "        vocab_size: an integer indicating the number of words desired for the\n",
    "                     bag of words vocab set\n",
    "        these are provisional arguments, please feel free to change if you need.\n",
    "\n",
    "    Outputs:\n",
    "        a vocab_size x dim (see below) array which contains the cluster centers \n",
    "        that result from the K Means clustering.\n",
    "\n",
    "    You need to generate image features using the any of the feature description\n",
    "    function, such as SIFT, HOG etc. The documentation is available here:\n",
    "    http://scikit-image.org/doc\n",
    "    s/dev/api/skimage.feature.html\n",
    "    Or you can have a look on OpenCV documentation here:\n",
    "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html\n",
    "\n",
    "    It is up to you to choose your feature width and other parameters. Choose\n",
    "    values that generate reasonably-sized feature vectors and produce good\n",
    "    classification results.\n",
    "\n",
    "    ONE MORE THING\n",
    "    If we returned all the features we found as our vocabulary, we would have an\n",
    "    absolutely massive vocabulary. That would make matching inefficient AND\n",
    "    inaccurate! So we use K Means clustering to find a much smaller (vocab_size)\n",
    "    number of representative points. We recommend using sklearn.cluster.KMeans\n",
    "    (or sklearn.cluster.MiniBatchKMeans if KMeans takes too long for you) to do this. \n",
    "    Note that this can take a VERY LONG TIME to complete (upwards of ten minutes \n",
    "    for large numbers of features and large max_iter), so set the max_iter argument\n",
    "    to something low (we used 100) and be patient. You may also find success setting\n",
    "    the \"tol\" argument (see documentation for details)\n",
    "    '''\n",
    "\n",
    "    dim = 128  # for SIFT descriptors, the dim is 128, please change it for others\n",
    "    vocab = np.zeros((vocab_size, dim))\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                     #\n",
    "    ############################################################################\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    img = cv2.imread(image_paths[0])\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, descriptors = sift.detectAndCompute(gray, None)\n",
    "    \n",
    "    for i in image_paths[1:]:\n",
    "        img = cv2.imread(i)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, des = sift.detectAndCompute(gray, None)\n",
    "        descriptors = np.concatenate([descriptors, des])\n",
    "      \n",
    "    \n",
    "    vocab = KMeans(n_clusters=vocab_size, max_iter=100).fit(descriptors).cluster_centers_\n",
    "    \n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laJnctLuqS6I"
   },
   "source": [
    "## 4. Build Histograms\n",
    "\n",
    "Now we are ready to represent our training and testing images as histograms of visual words. For each image we will sample many local visual descriptors using the same settings you used while creating the visual vocabulary. Instead of storing hundreds of visual descriptors, we simply count how many visual descriptors fall into each cluster in our visual word vocabulary. This is done by finding the nearest neighbor kmeans centroid for every visual feature. Thus, if we have a vocabulary of 50 visual words, and we detect 220 visual features in an image, our bag of visual feature representation will be a histogram of 50 dimensions where each bin counts how many times a feature descriptor was assigned to that cluster and sums to 220. The histogram should be normalized so that image size does not dramatically change the bag of feature magnitude. See below the code of `get_bags_of_words()` for more details. **[15 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rKXfySQ6qW2M"
   },
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm\n",
    "\n",
    "def get_bags_of_words(image_paths):\n",
    "    '''\n",
    "    This function should take in a list of image paths and calculate a bag of\n",
    "    words histogram for each image, then return those histograms in an array.\n",
    "    This feature representation is described in the handout, lecture\n",
    "    materials, and Szeliski chapter 14. You will want to construct visual features \n",
    "    here in the same way you did in build_vocabulary() (except for possibly \n",
    "    changing the sampling rate) and then assign each local feature to its nearest \n",
    "    cluster center and build a histogram indicating how many times each cluster \n",
    "    was used. Don't forget to normalize the histogram, or else a larger image with \n",
    "    more visual features will look very different from a smaller version of the same\n",
    "    image.\n",
    "\n",
    "    Inputs:\n",
    "        image_paths: A Python list of strings, where each string is a complete\n",
    "                     path to one image on the disk.\n",
    "        these are provisional arguments, please feel free to change if you need.\n",
    "\n",
    "    Outputs:\n",
    "        An nxd numpy matrix, where n is the number of images in image_paths and\n",
    "        d is size of the histogram built for each image.\n",
    "\n",
    "    Use the same image description function to extract feature vectors as before (see\n",
    "    build_vocabulary). It is important that you use the same descriptor settings for\n",
    "    both build_vocabulary and get_bags_of_words! Otherwise, you will end up\n",
    "    with different feature representations between your vocab and your test\n",
    "    images, and you won't be able to match anything at all!\n",
    "\n",
    "    After getting the feature vectors for an image, you will build up a\n",
    "    histogram that represents what words are contained within the image.\n",
    "    For each feature, find the closest vocab word, then add 1 to the histogram\n",
    "    at the index of that word. For example, if the closest vector in the vocab\n",
    "    is the 103rd word, then you should add 1 to the 103rd histogram bin. Your\n",
    "    histogram should have as many bins as there are vocabulary words.\n",
    "\n",
    "    Suggested functions: scipy.spatial.distance.cdist, np.argsort,\n",
    "                         np.linalg.norm, although there is no restriction on the \n",
    "                         usability of functions here.\n",
    "    '''\n",
    "\n",
    "    # load vocabulary\n",
    "    vocab = np.load('vocab.npy')\n",
    "    print('Loaded vocab from file.')\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: YOUR CODE HERE                                                     #\n",
    "    ############################################################################\n",
    "\n",
    "    # dummy features variable\n",
    "    feats = np.zeros((1,len(vocab)))\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    img = cv2.imread(image_paths[0])\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, descriptors = sift.detectAndCompute(gray, None)\n",
    "    distances = cdist(descriptors, vocab, 'euclidean')\n",
    "    for d in range(len(descriptors)):\n",
    "      feats[0][np.argsort(distances[d])[0]] += 1\n",
    "    \n",
    "    feats[0] = normalize(feats[0])\n",
    "    \n",
    "\n",
    "    for i in image_paths[1:]:\n",
    "      img = cv2.imread(i)\n",
    "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "      _, descriptors = sift.detectAndCompute(gray, None)\n",
    "      hist = np.zeros(len(vocab))\n",
    "      distances = cdist(descriptors, vocab, 'euclidean')\n",
    "      for d in range(len(descriptors)):\n",
    "        hist[np.argsort(distances[d])[0]] += 1\n",
    "\n",
    "      hist = normalize(hist)\n",
    "      feats = np.vstack([feats, hist])\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-Oz2WXkrIX7"
   },
   "source": [
    "## 5. Bag of Visual Words Representation with Nearest Neighbour Classifiers\n",
    "\n",
    "You should now measure how well your bag of visual words representation works when paired with a nearest neighbor classifier. There are many design decisions and free parameters for the bag of visual representation (number of clusters, sampling density, sampling scales, descriptor parameters, etc.) so accuracy might vary from 50% to 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "Yp9WtPUJrXvG",
    "outputId": "8e64488e-6b01-437c-b364-44eb8aa58b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting paths and labels for all train and test data.\n",
      "Using bag_of_words representation for images.\n",
      "Computing vocab from training images.\n",
      "Loaded vocab from file.\n",
      "Loaded vocab from file.\n",
      "Using nearest_neighbor classifier to predict test set categories.\n",
      "Creating results_webpage/index.html, thumbnails, and confusion matrix.\n",
      "Making results_webpage directory.\n",
      "Making thumbnails directory.\n",
      "Accuracy (mean of diagonal of confusion matrix) is 37.800%\n",
      "Wrote results page to results_webpage/index.html.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "projSceneRecBoW('bag_of_words', 'nearest_neighbor', 'False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVgRjpl3qlDN"
   },
   "source": [
    "## 6. Train 1-vs-all SVMs\n",
    "\n",
    "The last task is to train 1-vs-all linear SVMS to operate in the bag of visual feature space. Linear classifiers are one of the simplest possible learning models. The feature space is partitioned by a learned hyperplane and test cases are categorized based on which side of that hyperplane they fall on. Despite this model being far less expressive than the nearest neighbor classifier, it will often perform better. For example, maybe in our bag of visual words representation 40 of the 50 visual words are uninformative. They simply don't help us make a decision about whether an image is a 'forest' or a 'bedroom'. Perhaps they represent smooth patches, gradients, or step edges which occur in all types of scenes. The prediction from a nearest neighbor classifier will still be heavily influenced by these frequent visual words, whereas a linear classifier can learn that those dimensions of the feature vector are less relevant and thus downweight them when making a decision. There are numerous methods to learn linear classifiers but we will find linear decision boundaries with a support vector machine. You do not have to implement the support vector machine. However, linear classifiers are inherently binary and we have a 15-way classification problem. To decide which of 15 categories a test case belongs to, you will train 15 binary, 1-vs-all SVMs. 1-vs-all means that each classifier will be trained to recognize 'forest' vs 'non-forest', 'kitchen' vs 'non-kitchen', etc. All 15 classifiers will be evaluated on each test case and the classifier which is most confidently positive \"wins\". E.g. if the 'kitchen' classifier returns a score of -0.2 (where 0 is on the decision boundary), and the 'forest' classifier returns a score of -0.3, and all of the other classifiers are even more negative, the test case would be classified as a kitchen even though none of the classifiers put the test case on the positive side of the decision boundary. When learning an SVM, you have a free parameter 'lambda' which controls how strongly regularized the model is. Your accuracy will be very sensitive to lambda, so be sure to test many values. See svm_classify() for more details.\n",
    "\n",
    "Now you can evaluate the bag of visual words representation paired with 1-vs-all linear SVMs. Accuracy should be from 60% to 70% depending on the parameters. See the `svm_classify()` code below for more details. You can do better still if you implement extra credit suggestions below. **[10 marks]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YI4ue4ussFho"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    '''\n",
    "    This function will predict a category for every test image by training\n",
    "    15 many-versus-one linear SVM classifiers on the training data, then\n",
    "    using those learned classifiers on the testing data.\n",
    "\n",
    "    Inputs:\n",
    "        train_image_feats: An nxd numpy array, where n is the number of training\n",
    "                           examples, and d is the image descriptor vector size.\n",
    "        train_labels: An nx1 Python list containing the corresponding ground\n",
    "                      truth labels for the training data.\n",
    "        test_image_feats: An mxd numpy array, where m is the number of test\n",
    "                          images and d is the image descriptor vector size.\n",
    "        these are provisional arguments, please feel free to change if you need.\n",
    "\n",
    "    Outputs:\n",
    "        An mx1 numpy array of strings, where each string is the predicted label\n",
    "        for the corresponding image in test_image_feats\n",
    "\n",
    "    We suggest you look at the sklearn.svm module, including the LinearSVC\n",
    "    class. With the right arguments, you can get a 15-class SVM as described\n",
    "    above in just one call! Be sure to read the documentation carefully.\n",
    "\n",
    "    Useful functions:\n",
    "        sklearn LinearSVC\n",
    "        http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "        svm.fit(X, y), although there is no restriction on the usability of \n",
    "        functions here.\n",
    "    '''\n",
    "\n",
    "    # categories\n",
    "    categories = list(set(train_labels))\n",
    "\n",
    "    # construct 1 vs all SVMs for each category\n",
    "    svms = {cat: LinearSVC(random_state=0, tol=1e-3, loss='hinge', C=5) for cat in categories}\n",
    "    svm = LinearSVC(random_state=0, tol=1e-3, loss='hinge', C=5, multi_class='ovr')\n",
    "\n",
    "    svm.fit(train_image_feats, train_labels)\n",
    "\n",
    "    test_labels = svm.predict(test_image_feats)\n",
    "    print(test_labels)\n",
    "\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19M6i5s0uFjo"
   },
   "source": [
    "## 7. Remaining 20 Marks\n",
    "The remaining **20 marks** will be on the obtained performance of your system (detection, description, feature quantization/encoding, classification). You can interprete your results from the following table.\n",
    "\n",
    "| Accuracy | Interpretation   |\n",
    "|------|------|\n",
    "|   ~0.00  | Something is broken.|\n",
    "|   ~0.07  | Your performance is equal to chance. Something is broken or you ran the starter code unchanged.|\n",
    "|   ~0.20  | Rough performance with tiny images and nearest neighbor classifier. Performance goes up a few percentage points with K-NN instead of 1-NN.|\n",
    "|   ~0.20  | Rough performance with tiny images and linear SVM classifier. Although the accuracy is about the same as nearest neighbor, the confusion matrix is very different.|\n",
    "|   ~0.40  | Rough performance with bag of word and nearest neighbor classifier. Can reach .60 with K-NN and different distance metrics.|\n",
    "|   ~0.50  | You've gotten things roughly correct with bag of word and a linear SVM classifier.|\n",
    "|   ~0.70  | You've also tuned your parameters well. E.g. number of clusters, SVM regularization, number of patches sampled when building vocabulary, size and step for dense features.|\n",
    "|   ~0.80  | You've added in spatial information somehow or you've added additional, complementary image features. This represents state of the art in [Lazebnik et al. 2006](http://www.cs.unc.edu/~lazebnik/publications/cvpr06b.pdf).|\n",
    "|   ~0.85  | You've done extremely well. This is the state of the art in the 2010 SUN database paper from fusing many features. Don't trust this number unless you actually measure many random splits.|\n",
    "|   ~0.90  | You used modern deep features trained on much larger image databases.|\n",
    "|   ~0.96  | You can beat a human at this task. This isn't a realistic number. Some accuracy calculation is broken or your classifier is cheating and seeing the test labels.|\n",
    "\n",
    "There are a number of ways how you can improve the obtained performance, some ways are as below. You are also free to explore and can find out ways to improve your results. **[20 marks]**\n",
    "- Train the SVM with more sophisticated kernels such as Gaussian/RBF, L1 or $\\chi^2$.\n",
    "- Add additional, complementary features (e.g. [gist descriptors](http://people.csail.mit.edu/torralba/code/spatialenvelope/) and [self-similarity descriptors](https://www.robots.ox.ac.uk/~vgg/software/SelfSimilarity/)) and have the classifier consider them all. \n",
    "- Add spatial information to your features by implementing the spatial pyramid feature representation described in [Lazebnik et al. 2006](http://www.cs.unc.edu/~lazebnik/publications/cvpr06b.pdf).\n",
    "- Use one of the more sophisticated feature encoding schemes analyzed in the comparative study of [Chatfield et al. 2011](https://www.robots.ox.ac.uk/~vgg/research/encoding_eval/) (Fisher, Super Vector, or LLC).\n",
    "- An excellent survey of feature encoding methods for bag of words models is available at [Chatfield et al. 2011](https://www.robots.ox.ac.uk/~vgg/publications/2011/Chatfield11/chatfield11.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrMqINqS4eIv"
   },
   "source": [
    "## 8. Execute the Pipeline\n",
    "In this section, you only need to change the settings for `feature` and `classifier` at the end and then execute the `projSceneRecBow` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "5i1_gYWa1x4_",
    "outputId": "5f65c64c-3ec6-4c75-b408-1bafb8e5b79c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting paths and labels for all train and test data.\n",
      "Using bag_of_words representation for images.\n",
      "Loaded vocab from file.\n",
      "Loaded vocab from file.\n",
      "Using support_vector_machine classifier to predict test set categories.\n",
      "['InsideCity' 'Suburb' 'InsideCity' ... 'Forest' 'Forest' 'Forest']\n",
      "Creating results_webpage/index.html, thumbnails, and confusion matrix.\n",
      "Accuracy (mean of diagonal of confusion matrix) is 50.600%\n",
      "Wrote results page to results_webpage/index.html.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change feature and classifier\n",
    "feature = 'bag_of_words'     # options: 'placeholder', 'tiny_image', 'bag_of_words'\n",
    "classifier = 'support_vector_machine'  # options: 'placeholder', 'nearest_neighbor', 'support_vector_machine'\n",
    "load_vocab = 'True'         # options: 'True' or 'False'\n",
    "projSceneRecBoW(feature, classifier, load_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_8b-AKIAZJQ"
   },
   "source": [
    "## 9. Report\n",
    "\n",
    "* For this project, you must write a **project report** in PDF (**maximum 4 pages**). There will not be any penalty for crossing the page limit of the report, but the examiners are not obligated to look beyond the first 4 pages. In the report you will describe your algorithm and any decisions you made to write your algorithm a particular way. Discuss any extra task you did and show what contribution it had made on the results (e.g. performance with and without each extra task component).\n",
    "\n",
    "* You are required to report the accuracy you achieved for the three recognition pipelines above (tiny images + nearest neighbor, bag of visual words + nearest neighbor, and bag of visual words + 1 vs all linear SVM). The accuracy number reported by the starter code -- the average of the diagonal of the confusion matrix -- will suffice. However, for your best performing recognition setup you should include the full confusion matrix and the table of classifier results produced by the starter code (have a look within the results webpage folder). You can simply copy and paste the images into your writeup.\n",
    "\n",
    "**[20 marks]**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECMM426_ECMM441_Bag_of_Visual_Words_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
